{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yuvalbarkan/deep_learning/blob/main/HW1_ID.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnJUALElET8I"
      },
      "source": [
        "# Exercise 1: Linear Image Classifier\n",
        "\n",
        "In this exercise you will implement a linear image classifier while getting familiar with `numpy` and the benefits of vectorized operations in Python. This exercise has 2 parts:\n",
        "\n",
        "1. Implementing loss functions, calculating gradients and implementing gradient descent.\n",
        "2. Training and evaluating several classifiers.\n",
        "\n",
        "## Submission guidelines:\n",
        "\n",
        "Your submission should only include this jupyter notebook named HW1_ID.ipynb.\n",
        "\n",
        "## Read the following instructions carefully:\n",
        "\n",
        "1. This jupyter notebook contains all the step by step instructions needed for this exercise.\n",
        "2. Write **efficient vectorized** code whenever instructed. \n",
        "3. You are responsible for the correctness of your code and should add as many tests as you see fit. Tests will not be graded nor checked.\n",
        "4. Do not change the functions we provided you. \n",
        "4. Write your functions in the instructed python modules only. All the logic you write is imported and used using this jupyter notebook. You are allowed to add functions as long as they are located in the python modules and are imported properly.\n",
        "5. You are allowed to use functions and methods from the [Python Standard Library](https://docs.python.org/3/library/) and [numpy](https://www.numpy.org/devdocs/reference/) only. Any other imports are forbidden.\n",
        "6. Your code must run without errors.\n",
        "7. Answers to qualitative questions should be written in **markdown** cells (with $\\LaTeX$ support).\n",
        "8. **TIP: When there is a TODO before a missing code segment (or function), you can continue without implementing it right away; you will be referred to the missing segment later in the exercise.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_r1L4UklET8K"
      },
      "source": [
        "Q: What files do we need to upload to moodle?\n",
        "\n",
        "A: You should fill in the missing parts in this Jupyter notebook and then submit it via moodle (without any additional files).\n",
        "\n",
        "Q: How do I make sure everything works before I submit?\n",
        "\n",
        "A: You should restart your kernel and rerun all cells. Make sure you get the desired output and that you meet exercise requirements. **This is an important step. You should include your desired outputs in the output cells to make your code easier to understand.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UV9XOToVGvLZ"
      },
      "source": [
        "#### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-10-29T15:21:00.158255Z",
          "start_time": "2022-10-29T15:20:59.290618Z"
        },
        "id": "iLXvPpILET8K"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import urllib.request\n",
        "import tarfile\n",
        "import zipfile\n",
        "from random import randrange\n",
        "from functools import partial\n",
        "import itertools\n",
        "import time\n",
        "\n",
        "# specify the way plots behave in jupyter notebook\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (5.0, 3.0) # set default size of plots\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "plt.rcParams['image.cmap'] = 'gray'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "BtYsC3I7ET8L"
      },
      "source": [
        "# Data preprocessing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "-WovI1B4Gxwp"
      },
      "source": [
        "## Data download and processing Helper Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-10-29T15:21:00.767100Z",
          "start_time": "2022-10-29T15:21:00.754066Z"
        },
        "code_folding": [],
        "hidden": true,
        "id": "sC6iBqvmGYsn"
      },
      "outputs": [],
      "source": [
        "def maybe_download_and_extract(url, download_dir):\n",
        "    \"\"\"\n",
        "    Download and extract the data if it doesn't already exist.\n",
        "    Assumes the url is a tar-ball file.\n",
        "    :param url:\n",
        "        Internet URL for the tar-file to download.\n",
        "        Example: \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
        "    :param download_dir:\n",
        "        Directory where the downloaded file is saved.\n",
        "        Example: \"data/CIFAR-10/\"\n",
        "    :return:\n",
        "        Nothing.\n",
        "    \"\"\"\n",
        "\n",
        "    # Filename for saving the file downloaded from the internet.\n",
        "    # Use the filename from the URL and add it to the download_dir.\n",
        "    filename = url.split('/')[-1]\n",
        "    file_path = os.path.join(download_dir, filename)\n",
        "\n",
        "    # Check if the file already exists.\n",
        "    # If it exists then we assume it has also been extracted,\n",
        "    # otherwise we need to download and extract it now.\n",
        "    if not os.path.exists(file_path):\n",
        "        # Check if the download directory exists, otherwise create it.\n",
        "        if not os.path.exists(download_dir):\n",
        "            os.makedirs(download_dir)\n",
        "\n",
        "        # Download the file from the internet.\n",
        "        print(\"Downloading, This might take several minutes.\")\n",
        "        last_update_time = time.time()\n",
        "        file_path, _ = urllib.request.urlretrieve(url=url,\n",
        "                                                  filename=file_path)\n",
        "\n",
        "        print()\n",
        "        print(\"Download finished. Extracting files.\")\n",
        "\n",
        "        if file_path.endswith(\".zip\"):\n",
        "            # Unpack the zip-file.\n",
        "            zipfile.ZipFile(file=file_path, mode=\"r\").extractall(download_dir)\n",
        "        elif file_path.endswith((\".tar.gz\", \".tgz\")):\n",
        "            # Unpack the tar-ball.\n",
        "            tarfile.open(name=file_path, mode=\"r:gz\").extractall(download_dir)\n",
        "\n",
        "        print(\"Done.\")\n",
        "    else:\n",
        "        print(\"Data has apparently already been downloaded and unpacked.\")\n",
        "        print(\"If not, delete the dataset folder and try again.\")\n",
        "\n",
        "def load_CIFAR_batch(filename):\n",
        "    ''' load single batch of cifar '''\n",
        "    with open(filename, 'rb') as f:\n",
        "        datadict = pickle.load(f, encoding = 'latin1')\n",
        "        X = datadict['data']\n",
        "        Y = datadict['labels']\n",
        "        X = X.reshape(10000, 3, 32, 32).transpose(0, 2, 3, 1).astype(\"float\")\n",
        "        Y = np.array(Y)\n",
        "        return X, Y\n",
        "\n",
        "def load(ROOT):\n",
        "    ''' load all of cifar '''\n",
        "    xs = []\n",
        "    ys = []\n",
        "    for b in range(1, 6):\n",
        "        f = os.path.join(ROOT, 'data_batch_%d' % (b, ))\n",
        "        X, Y = load_CIFAR_batch(f)\n",
        "        xs.append(X)\n",
        "        ys.append(Y)\n",
        "    Xtr = np.concatenate(xs)\n",
        "    Ytr = np.concatenate(ys)\n",
        "    del X, Y\n",
        "    Xte, Yte = load_CIFAR_batch(os.path.join(ROOT, 'test_batch'))\n",
        "    return Xtr, Ytr, Xte, Yte"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "0qfmwr8lG3S8"
      },
      "source": [
        "## Data Download\n",
        "\n",
        "The next cell will download and extract CIFAR-10 into `datasets/cifar10/`. The CIFAR-10 dataset consists of 60,000 32x32 color images in 10 classes, with 6,000 images per class. There are 50,000 training images and 10,000 test images. The dataset is divided into five training batches and one test batch, each with 10,000 images. The test batch contains exactly 1,000 randomly-selected images from each class.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-10-29T15:21:01.686261Z",
          "start_time": "2022-10-29T15:21:01.675796Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "hidden": true,
        "id": "eBbBnxJpET8L",
        "outputId": "9152e6aa-b9dc-4ef1-a618-79ac24fe7002"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading, This might take several minutes.\n",
            "\n",
            "Download finished. Extracting files.\n",
            "Done.\n"
          ]
        }
      ],
      "source": [
        "# this cell will download the data if it does not exists\n",
        "URL = \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
        "PATH = 'datasets/cifar10/' # the script will create required directories\n",
        "maybe_download_and_extract(URL, PATH) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "AIyo61vC-Dmv"
      },
      "source": [
        "## Data Preprocessing Part 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "ersTr2wB-RAd"
      },
      "source": [
        "We have included several image processing functions. Notice the following in particular: we created an additional validation dataset you need to use for hyperparameter optimization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-10-29T15:21:05.097768Z",
          "start_time": "2022-10-29T15:21:03.341893Z"
        },
        "hidden": true,
        "id": "NFUWV8LnET8M"
      },
      "outputs": [],
      "source": [
        "CIFAR10_PATH = os.path.join(PATH, 'cifar-10-batches-py')\n",
        "X_train, y_train, X_test, y_test = load(CIFAR10_PATH) # load the entire data\n",
        "\n",
        "# taking only two classes from the dataset\n",
        "X_train = X_train[np.logical_or(y_train == 0, y_train == 1)]\n",
        "y_train = y_train[np.logical_or(y_train == 0, y_train == 1)]\n",
        "X_test = X_test[np.logical_or(y_test == 0, y_test == 1)]\n",
        "y_test = y_test[np.logical_or(y_test == 0, y_test == 1)]\n",
        "\n",
        "# define a splitting for the data\n",
        "num_training = 10000\n",
        "num_validation = 1000\n",
        "num_testing = 1000\n",
        "\n",
        "mask = range(num_training)\n",
        "X_train = X_train[mask]\n",
        "y_train = y_train[mask]\n",
        "# portion from the test dataset a validation dataset for hyperparameter optimization\n",
        "mask = range(num_validation)\n",
        "X_val = X_test[mask]\n",
        "y_val = y_test[mask]\n",
        "# test dataset, without overlap with train/validation\n",
        "mask = range(num_validation, num_validation+num_testing)\n",
        "X_test = X_test[mask]\n",
        "y_test = y_test[mask]\n",
        "\n",
        "# float64\n",
        "X_train = X_train.astype(np.float64)\n",
        "X_val = X_val.astype(np.float64)\n",
        "X_test = X_test.astype(np.float64)\n",
        "\n",
        "classes = ('plane', 'car')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-10-29T15:21:05.911006Z",
          "start_time": "2022-10-29T15:21:05.755837Z"
        },
        "hidden": true,
        "id": "Daycmt2x5cVS"
      },
      "outputs": [],
      "source": [
        "def get_batch(X, y, n=1000):\n",
        "    rand_items = np.random.randint(0, X.shape[0], size=n)\n",
        "    images = X[rand_items]\n",
        "    labels = y[rand_items]\n",
        "    return images, labels\n",
        "\n",
        "def make_random_grid(x, y, n=4, convert_to_image=True, random_flag=True):\n",
        "    if random_flag:\n",
        "        rand_items = np.random.randint(0, x.shape[0], size=n)\n",
        "    else:\n",
        "        rand_items = np.arange(0, x.shape[0])\n",
        "    images = x[rand_items]\n",
        "    labels = y[rand_items]\n",
        "    if convert_to_image:\n",
        "        grid = np.hstack(np.array([np.asarray((vec_2_img(i) + mean_image), dtype=np.int64) for i in images]))\n",
        "    else:\n",
        "        grid = np.hstack(np.array([np.asarray(i, dtype=np.int64) for i in images]))\n",
        "    print('\\t'.join('%9s' % classes[labels[j]] for j in range(n)))\n",
        "    return grid\n",
        "\n",
        "def vec_2_img(x):\n",
        "    x = np.reshape(x[:-1], (32, 32, 3))\n",
        "    return x\n",
        "\n",
        "X_batch, y_batch = get_batch(X_test, y_test, 100)\n",
        "plt.imshow(make_random_grid(X_batch, y_batch, n=4, convert_to_image=False));"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "RhismcUO9-xk"
      },
      "source": [
        "## Data Preprocessing Part 2\n",
        "\n",
        "We have included several image processing functions. Notice the following in particular: We subtracted the mean from all the images in order to ignore illumination conditions while keeping the content of the image. Next, we flattened the images from a tensor of shape (32x32x3) to a vector with 3072 features (pixel values) so we would be able to use a simple matrix multiplication. Finally, we concatenated each image vector with an additional feature to account for the bias. This is known as the bias trick. \n",
        "\n",
        "Make sure you understand this image processing pipeline before diving into the rest of the code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-10-29T15:21:06.836277Z",
          "start_time": "2022-10-29T15:21:06.555473Z"
        },
        "hidden": true,
        "id": "C9YCYi5KZhXD"
      },
      "outputs": [],
      "source": [
        "# Final data preprocessing\n",
        "# subtract the mean from all the images in the batch\n",
        "mean_image = np.mean(X_train, axis=0)\n",
        "X_train -= mean_image\n",
        "X_val -= mean_image\n",
        "X_test -= mean_image\n",
        "\n",
        "# flatten all the images in the batch (make sure you understand why this is needed)\n",
        "X_train = np.reshape(X_train, newshape=(X_train.shape[0], -1))\n",
        "X_val = np.reshape(X_val, newshape=(X_val.shape[0], -1)) \n",
        "X_test = np.reshape(X_test, newshape=(X_test.shape[0], -1)) \n",
        "\n",
        "# add a bias term to all images in the batch\n",
        "X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))]) \n",
        "X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))]) \n",
        "X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))]) \n",
        "\n",
        "print(f\"Shape of Training Set: {X_train.shape}\")\n",
        "print(f\"Shape of Validation Set: {X_val.shape}\")\n",
        "print(f\"Shape of Test Set: {X_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BX0bVW0ET8N"
      },
      "source": [
        "# Linear classifier: mapping images to scores\n",
        "\n",
        "During this exercise, we will maintain a python class with basic functionality (such as training the model). the linear classifiers we will build (perceptron, logistic regression) will inherit some functionality from that class and will change several functions (such as the loss function, for example). Read the code in the next cell and make sure you understand it. You might also find this [short classes in python tutorial](https://www.hackerearth.com/practice/python/object-oriented-programming/classes-and-objects-i/tutorial/) useful.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-10-29T15:21:15.523094Z",
          "start_time": "2022-10-29T15:21:15.502785Z"
        },
        "id": "FXAVk7l6LVty"
      },
      "outputs": [],
      "source": [
        "class LinearClassifier(object):\n",
        "    def __init__(self, X, y):\n",
        "        \"\"\"\n",
        "        Class constructor. Use this method to initiate the parameters of\n",
        "        your model (W)\n",
        "        *** Subclasses will override this. ***\n",
        "\n",
        "        Inputs:\n",
        "        - X: array of data\n",
        "        - y: 1-dimensional array of length N with binary labels\n",
        "\n",
        "        This function has no return value\n",
        "\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Use the weight of the classifier to predict a label. \n",
        "        *** Subclasses will override this. ***\n",
        "\n",
        "        Input: 2D array of size (num_instances, num_features).\n",
        "        Output: 1D array of class predictions (num_instances, 1). \n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def calc_accuracy(self, X, y):\n",
        "        \"\"\"\n",
        "        Calculate the accuracy on a dataset as the percentage of instances \n",
        "        that are classified correctly. \n",
        "\n",
        "        Inputs:\n",
        "        - W: array of weights\n",
        "        - X: array of data\n",
        "        - y: 1-dimensional array of length N with binary labels\n",
        "        Returns:\n",
        "        - accuracy as a single float\n",
        "        \"\"\"\n",
        "\n",
        "        accuracy = 0.0\n",
        "        ###########################################################################\n",
        "        # TODO: Implement this method.                                            #\n",
        "        ###########################################################################\n",
        "        #                          START OF YOUR CODE                             #\n",
        "        ###########################################################################\n",
        "\n",
        "        \n",
        "        \n",
        "        ###########################################################################\n",
        "        #                           END OF YOUR CODE                              #\n",
        "        ###########################################################################\n",
        "\n",
        "        return accuracy\n",
        "\n",
        "\n",
        "    def train(self, X, y, learning_rate=1e-3, num_iters=100, batch_size=200, verbose=False):\n",
        "        #########################################################################\n",
        "        # TODO:                                                                 #\n",
        "        # Sample batch_size elements from the training data and their           #\n",
        "        # corresponding labels to use in every iteration.                       #\n",
        "        # Store the data in X_batch and their corresponding labels in           #\n",
        "        # y_batch                                                               #\n",
        "        #                                                                       #\n",
        "        # Hint: Use np.random.choice to generate indices. Sampling with         #\n",
        "        # replacement is faster than sampling without replacement.              #\n",
        "        #                                                                       #\n",
        "        # Next, calculate the loss and gradient and update the weights using    #\n",
        "        # the learning rate. Use the loss_history array to save the loss on     #\n",
        "        # iteration to visualize the loss.                                      #\n",
        "        #########################################################################\n",
        "        num_instances = X.shape[0]\n",
        "        loss_history = []\n",
        "        loss = 0.0\n",
        "        for i in range(num_iters):\n",
        "            X_batch = None\n",
        "            y_batch = None\n",
        "            ###########################################################################\n",
        "            # Create X_batch and y_batch. Call the loss method to get the loss value  # \n",
        "            # and grad (the loss function is being override, see the loss             #\n",
        "            # function return values).                                                #\n",
        "            # Finally, append each of the loss values created in each iteration       #\n",
        "            # to loss_history.                                                        #\n",
        "            ###########################################################################\n",
        "            #                          START OF YOUR CODE                             #\n",
        "            ###########################################################################\n",
        "\n",
        "            \n",
        "            \n",
        "            \n",
        "            \n",
        "            \n",
        "            ###########################################################################\n",
        "            #                           END OF YOUR CODE                              #\n",
        "            ###########################################################################\n",
        "            # TODO:                                                                   #\n",
        "            # Perform parameter update                                                #\n",
        "            # Update the weights using the gradient and the learning rate.            #\n",
        "            ###########################################################################\n",
        "            #                          START OF YOUR CODE                             #                                                         #\n",
        "            ###########################################################################\n",
        "\n",
        "            \n",
        "            ###########################################################################\n",
        "            #                       END OF YOUR CODE                                  #\n",
        "            ###########################################################################\n",
        "\n",
        "            if verbose and i % 100 == 0:\n",
        "                print ('iteration %d / %d: loss %f' % (i, num_iters, loss))\n",
        "\n",
        "        return loss_history\n",
        "\n",
        "\n",
        "    def loss(self, X, y):\n",
        "        \"\"\"\n",
        "        Compute the loss function and its derivative. \n",
        "        Subclasses will override this.\n",
        "        Inputs:\n",
        "        - X_batch: A numpy array of shape (N, D) containing a minibatch of N\n",
        "          data points; each point has dimension D.\n",
        "        - y_batch: A numpy array of shape (N,) containing labels for the minibatch.\n",
        "        Returns: A tuple containing:\n",
        "        - loss as a single float\n",
        "        - gradient with respect to self.W; an array of the same shape as W\n",
        "        \"\"\"\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9WqAhlmLMGU"
      },
      "source": [
        "## Linear perceptron\n",
        "Our first linear classifier will include a linear function that maps images to scores:\n",
        "\n",
        "$$\n",
        "f(x_i; W, b) = W\\cdot x_i + b\n",
        "$$\n",
        "\n",
        "As you learned in class, this linear classifier takes an input image $x_i$ and outputs a class score. Your goal is to **learn** the parameters $W$ and $b$ to best classify the images according to the provided labels. The linear perceptron is set up so that the perceptron learn to map the correct class for each image such that it will have a score higher than the incorrect class.\n",
        "\n",
        "Read the next code cell. The constructor of the `LinearPerceptron` class takes as input the dataset and labels in order to create appropriate parameters. Notice we are using the bias trick and only use the matrix `w` for convenience. Since we already have a (random) model, we can start predicting classes on images. Complete the method `predict` in the `LinearPerceptron` class. **(5 Points)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-10-29T15:21:38.587623Z",
          "start_time": "2022-10-29T15:21:38.576532Z"
        },
        "id": "cfLTGvYILcJw"
      },
      "outputs": [],
      "source": [
        "class LinearPerceptron(LinearClassifier):\n",
        "    # Classifier that uses Perceptron loss\n",
        "\n",
        "    def __init__(self, X, y):\n",
        "        ###########################################################################\n",
        "        # Initiate the parameters of your model.                                  #\n",
        "        # You can assume y takes values 0...K-1 where K is number of classes      #\n",
        "        ###########################################################################\n",
        "        #                          START OF YOUR CODE                             # \n",
        "        ###########################################################################\n",
        "\n",
        "        \n",
        "        \n",
        "        ###########################################################################\n",
        "        #                           END OF YOUR CODE                              #\n",
        "        ###########################################################################\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        y_pred = None\n",
        "        ###########################################################################\n",
        "        # Implement this method.                                                  #\n",
        "        ###########################################################################\n",
        "        #                          START OF YOUR CODE                             # \n",
        "        ###########################################################################\n",
        "\n",
        "        \n",
        "        \n",
        "        ###########################################################################\n",
        "        #                           END OF YOUR CODE                              #\n",
        "        ###########################################################################\n",
        "        return y_pred\n",
        "\n",
        "\n",
        "    def loss(self, X_batch, y_batch):\n",
        "        # perceptron_loss_vectorized will be implemented later\n",
        "        return perceptron_loss_vectorized(self.W, X_batch, y_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-10-29T15:21:40.039042Z",
          "start_time": "2022-10-29T15:21:39.994118Z"
        },
        "id": "2cb3cgLeET8N"
      },
      "outputs": [],
      "source": [
        "classifier = LinearPerceptron(X_train, y_train)\n",
        "y_pred = classifier.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-10-29T15:21:41.067042Z",
          "start_time": "2022-10-29T15:21:40.894184Z"
        },
        "id": "w2Cv7adkET8N"
      },
      "outputs": [],
      "source": [
        "X_batch, y_batch = get_batch(X_train, y_train, 4)\n",
        "plt.imshow(make_random_grid(X_batch, y_batch, convert_to_image=True))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-10-29T15:21:42.911853Z",
          "start_time": "2022-10-29T15:21:42.897375Z"
        },
        "id": "BKUh3OFFET8N"
      },
      "outputs": [],
      "source": [
        "# predictions\n",
        "print(' '.join('%9s' % classes[y_pred[j]] for j in range(4)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-1RXh-lET8O"
      },
      "source": [
        "## Evaluation \n",
        "\n",
        "Complete the class method `calc_accuracy`. **(5 Points)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-10-29T15:23:38.099891Z",
          "start_time": "2022-10-29T15:23:38.029739Z"
        },
        "id": "IiSJ5-wvET8O"
      },
      "outputs": [],
      "source": [
        "print(\"model accuracy: \", classifier.calc_accuracy(X_train, y_train))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAFp3MOYET8O"
      },
      "source": [
        "**Explain why the accuracy on the training dataset is around 50% (remember, the model is not trained yet). **(5 Points)**** \n",
        "\n",
        "Answer: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuFKR-UKET8O"
      },
      "source": [
        "## Perceptron loss\n",
        "\n",
        "Your code for this section will all be written in the next cell. In this section, we write and test code outside the classes for convenience. Notice the loss method for each class is just a call for the loss function written in the next cell. Once you are finished with implementation, everything should work.\n",
        "\n",
        "First, complete the function `perceptron_loss_naive`. This function takes as input the weights, data, labels and outputs the calculated loss as a single number and the gradients with respect to W.  **(15 points)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-10-29T15:23:57.192000Z",
          "start_time": "2022-10-29T15:23:57.177408Z"
        },
        "id": "McEGmknNYFXC"
      },
      "outputs": [],
      "source": [
        "def perceptron_loss_naive(W, X, y):\n",
        "    \"\"\"\n",
        "    Structured perceptron loss function, naive implementation (with loops)\n",
        "    Inputs:\n",
        "    - W: array of weights\n",
        "    - X: array of data\n",
        "    - y: 1-dimensional array of length N with labels 0...K-1, for K classes\n",
        "    Returns:\n",
        "    a tuple of:\n",
        "    - loss as single float\n",
        "    - gradient with respect to weights W; an array of same shape as W\n",
        "    \"\"\"\n",
        "    loss = 0.0\n",
        "    dW = np.zeros(W.shape) # initialize the gradient as zero\n",
        "    #############################################################################\n",
        "    # Compute the perceptron loss as learned in class. Start by iterating over  #\n",
        "    # over all instances and calculate the score and true score for each.       #\n",
        "    # Now, for each class determine if the prediction is correct and update the #\n",
        "    # loss over all mistakes.                                                   #\n",
        "    # Compute the gradient of the loss function and store it as dW.             #\n",
        "    # Rather that first computing the loss and then computing the derivative,   #\n",
        "    # it may be simpler to compute the derivative at the same time that the     #\n",
        "    # loss is being computed.                                                   #\n",
        "    #############################################################################\n",
        "\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    #############################################################################\n",
        "    #                             END OF YOUR CODE                              #\n",
        "    #############################################################################\n",
        "    return loss, dW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-10-29T15:23:58.297179Z",
          "start_time": "2022-10-29T15:23:58.278906Z"
        },
        "id": "tGcQrlGJET8O"
      },
      "outputs": [],
      "source": [
        "W = np.random.randn(3073, 1) * 0.0001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-10-29T15:23:59.483591Z",
          "start_time": "2022-10-29T15:23:59.450741Z"
        },
        "id": "UKmH67DEET8O"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "loss_naive, _ = perceptron_loss_naive(W, X_val, y_val)\n",
        "print ('loss: %f' % (loss_naive))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-Hh-R7bET8P"
      },
      "source": [
        "Once your code works, complete the function `perceptron_loss_vectorized` and compare the results of the two functions using the cell below. **(15 points)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-10-29T15:24:11.592697Z",
          "start_time": "2022-10-29T15:24:11.576004Z"
        },
        "id": "w1HLNSCWYIRK"
      },
      "outputs": [],
      "source": [
        "def perceptron_loss_vectorized(W, X, y):\n",
        "    \"\"\"\n",
        "    Vectorized version of perceptron_loss_naive. instead of loops, should use \n",
        "    numpy vectorization.\n",
        "\n",
        "    Inputs and outputs are the same as perceptron_loss_naive.\n",
        "    \"\"\"\n",
        "    loss = 0.0\n",
        "    dW = np.zeros(W.shape) # initialize the gradient as zero\n",
        "    #############################################################################\n",
        "    # Implement a vectorized version of the perceptron loss, storing the       #\n",
        "    # result in loss and the gradient in dW                                     #\n",
        "    #############################################################################\n",
        "\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    #############################################################################\n",
        "    #                             END OF YOUR CODE                              #\n",
        "    #############################################################################\n",
        "    return loss, dW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-10-29T15:24:12.996442Z",
          "start_time": "2022-10-29T15:24:12.949777Z"
        },
        "id": "_puMMu8vET8P",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "loss_vectorized, _ = perceptron_loss_vectorized(W, X_val, y_val)\n",
        "print ('loss: %f' % (loss_vectorized))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6oNHXGHET8P"
      },
      "source": [
        "You might not see big changes in time due to other computing factors. In big enough datasets it would be crucial to use the vectorized version.\n",
        "\n",
        "We have obtained an efficient function for loss and gradient calculation and we can now train our network. Complete the function `train` in the `LinearClassifier` class. (**15 points**)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-10-29T15:24:38.956156Z",
          "start_time": "2022-10-29T15:24:31.984850Z"
        },
        "id": "_QtvCxghET8P"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "perceptron = LinearPerceptron(X_train, y_train)\n",
        "loss_history = perceptron.train(X_train, y_train, learning_rate=1e-7, \n",
        "                                num_iters=1500, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-10-29T15:24:39.656566Z",
          "start_time": "2022-10-29T15:24:39.516553Z"
        },
        "id": "_jzvksCFET8P",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "plt.plot(loss_history)\n",
        "plt.xlabel('Iteration number')\n",
        "plt.ylabel('Loss value')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-10-29T15:24:40.310924Z",
          "start_time": "2022-10-29T15:24:40.267510Z"
        },
        "id": "eFNormxyET8P"
      },
      "outputs": [],
      "source": [
        "print(\"Training accuracy: \", perceptron.calc_accuracy(X_train, y_train))\n",
        "print(\"Testing accuracy: \", perceptron.calc_accuracy(X_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R277VSx8ET8P"
      },
      "source": [
        "## Hyperparameter optimization\n",
        "\n",
        "Your model should have improved from 50% accuracy to ~75% accuracy in a matter of seconds. Now, use the validation set to tune hyperparameters by training different models (using the training dataset) and evaluating the performance using the validation dataset. Save the results in a dictionary mapping tuples of the form `(learning_rate, batch_size)` to tuples of the form `(training_accuracy, validation_accuracy)`. Finally, you should evaluate the best model on the testing dataset. \n",
        "\n",
        "Use a small value for the number of iterations as you develop your code. Once you are confident that everything works, run it again for more iterations. **(5 points)**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-10-29T15:28:28.780040Z",
          "start_time": "2022-10-29T15:27:34.777137Z"
        },
        "id": "m_g5iSBQET8Q"
      },
      "outputs": [],
      "source": [
        "# You are encouraged to experiment with additional values\n",
        "learning_rates = [1e-7, 5e-6]\n",
        "batch_sizes = [1, 100, 200, 500]\n",
        "\n",
        "results = {}\n",
        "best_val = -1   # The highest validation accuracy that we have seen so far.\n",
        "best_perceptron = None # The LinearPerceptron object that achieved the highest validation rate.\n",
        "\n",
        "################################################################################\n",
        "#                            START OF YOUR CODE                                #\n",
        "################################################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "################################################################################\n",
        "#                              END OF YOUR CODE                                #\n",
        "################################################################################\n",
        "    \n",
        "# Print out results.\n",
        "for lr, batch_size in sorted(results):\n",
        "    train_accuracy, val_accuracy = results[(lr, batch_size)]\n",
        "    print ('lr %e batch_size %e train accuracy: %f val accuracy: %f' % (\n",
        "                lr, batch_size, train_accuracy, val_accuracy))\n",
        "    \n",
        "print ('best validation accuracy achieved during cross-validation: %f' % best_val)\n",
        "\n",
        "test_accuracy = best_perceptron.calc_accuracy(X_test, y_test)\n",
        "print ('linear perceptron on raw pixels final test set accuracy: %f' % test_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upoW3pQ1ET8Q"
      },
      "source": [
        "## Logistic regression\n",
        "\n",
        "Another choice for a binary classifier is the binary logistic regression classifier. Unlike the perceptron which treats the outputs as uncalibrated and possibly difficult to interpret scores for each class, the binary logistic regression classifier gives a slightly more intuitive output in the form of normalized class probabilities. In this classifier, the function mapping $f(x_i; W, b) = W\\cdot x_i + b$ stays unchanged but we now interpret these scores as the unnormalized log probabilities for each class and replace the perceptron loss with a cross-entropy loss. In this exercise, we will define our binary logistic regression classifier to have one input.       \n",
        "\n",
        "Read the next code cell. The constructor of the `LogisticRegression` class takes as input the dataset and labels in order to create appropriate parameters. Notice we are using the bias trick and only use the matrix `w` for convenience. Since we already have a (random) model, we can start predicting classes on images. Complete the method `predict` in the `LogisticRegression` class - remember you need to implement the sigmoid function before you can obtain predictions using your classifier. **(10 Points)**\n",
        "\n",
        "**Important note**: values passed to the `sigmoid` function can be arbitrarily large or small. When we take the exponent of such values, we might encounter extreme values that might *overflow*. This is known as numerical instability and you should always take care when you use exponent in your functions. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-10-29T15:30:35.229664Z",
          "start_time": "2022-10-29T15:30:35.184722Z"
        },
        "id": "Y9cU2sJ_X96d"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x):\n",
        "    \"\"\"\n",
        "    Numerically stable Sigmoid function.\n",
        "\n",
        "    Input: any unnormalized log probabilities vector\n",
        "    Output: normalized probabilities\n",
        "    \"\"\"\n",
        "    #############################################################################\n",
        "    # Implement the function                                                    #\n",
        "    #############################################################################\n",
        "\n",
        "    \n",
        "    \n",
        "    \n",
        "    #############################################################################\n",
        "    #                             END OF YOUR CODE                              #\n",
        "    #############################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-10-29T15:30:52.504993Z",
          "start_time": "2022-10-29T15:30:52.486057Z"
        },
        "id": "oGClf3GJLlBp"
      },
      "outputs": [],
      "source": [
        "class LogisticRegression(LinearClassifier):\n",
        "    # Classifer that uses sigmoid and binary cross entropy loss\n",
        "    def __init__(self, X, y):\n",
        "        self.W = None\n",
        "        ###########################################################################\n",
        "        # Initiate the parameters of your model.                                  #\n",
        "        ###########################################################################\n",
        "\n",
        "        \n",
        "        \n",
        "        ###########################################################################\n",
        "        #                           END OF YOUR CODE                              #\n",
        "        ###########################################################################\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        y_pred = None\n",
        "        ###########################################################################\n",
        "        # Implement this method.                                                  #\n",
        "        ###########################################################################\n",
        "\n",
        "        \n",
        "        \n",
        "        \n",
        "        ###########################################################################\n",
        "        #                           END OF YOUR CODE                              #\n",
        "        ###########################################################################\n",
        "        return y_pred\n",
        "\n",
        "\n",
        "    def loss(self, X_batch, y_batch):\n",
        "        # will be implemented later\n",
        "        return binary_cross_entropy(self.W, X_batch, y_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-10-29T15:30:53.132207Z",
          "start_time": "2022-10-29T15:30:53.121237Z"
        },
        "id": "BRrb-mb5ET8Q"
      },
      "outputs": [],
      "source": [
        "logistic = LogisticRegression(X_train, y_train)\n",
        "y_pred = logistic.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-10-29T15:30:53.866044Z",
          "start_time": "2022-10-29T15:30:53.729569Z"
        },
        "id": "jUb2tkIMET8Q"
      },
      "outputs": [],
      "source": [
        "X_batch, y_batch = get_batch(X_train, y_train, 4)\n",
        "plt.imshow(make_random_grid(X_batch, y_batch));"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-10-29T15:31:05.584563Z",
          "start_time": "2022-10-29T15:31:05.559650Z"
        },
        "id": "1I5T0LiBET8Q"
      },
      "outputs": [],
      "source": [
        "# predictions\n",
        "print(' '.join('%13s' % classes[y_pred[j]] for j in range(4)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-10-29T15:31:10.105426Z",
          "start_time": "2022-10-29T15:31:10.033927Z"
        },
        "id": "29GDDyzIET8Q"
      },
      "outputs": [],
      "source": [
        "print(\"model accuracy: \", logistic.calc_accuracy(X_train, y_train))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vpy2WTyGET8Q"
      },
      "source": [
        "## Binary cross-entropy\n",
        "\n",
        "Your code for this section will written in the next cell. \n",
        "\n",
        "Complete the function `binary_cross_entropy` using vectorized code. This function takes as input the weights, data, labels and outputs the calculated loss as a single number and the gradients with respect to W. (**20 points**)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-10-29T15:31:30.874716Z",
          "start_time": "2022-10-29T15:31:30.826315Z"
        },
        "id": "pfjEDW6n3mRu"
      },
      "outputs": [],
      "source": [
        "def binary_cross_entropy(W, X, y):\n",
        "    \"\"\"\n",
        "    Structured BCE loss function. Implement this function using vectorized code.\n",
        "    Inputs:\n",
        "    - W: array of weights\n",
        "    - X: array of data\n",
        "    - y: 1-dimensional array of length N with binary labels (0,1). \n",
        "    Returns:\n",
        "    a tuple of:\n",
        "    - loss as single float\n",
        "    - gradient with respect to weights W; an array of same shape as W\n",
        "    \"\"\"\n",
        "    loss = 0.0\n",
        "    dW = np.zeros(W.shape) # initialize the gradient as zero\n",
        "    #############################################################################\n",
        "    # Implement the function and store result in loss and the gradint in dW     #\n",
        "    # Note: in class you defined BCE that takes values from the range (-1,1).   #\n",
        "    # and the sigmoid function generally outputs values in the range (0,1).     #\n",
        "    # Make the proper adjustments for your code to work.                        #\n",
        "    #############################################################################\n",
        "\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    #############################################################################\n",
        "    #                             END OF YOUR CODE                              #\n",
        "    #############################################################################\n",
        "    return loss, dW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-10-29T15:31:34.082144Z",
          "start_time": "2022-10-29T15:31:34.066172Z"
        },
        "id": "EvLw8x4oET8Q"
      },
      "outputs": [],
      "source": [
        "W = np.random.randn(3073, 1) * 0.0001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-10-29T15:31:36.963381Z",
          "start_time": "2022-10-29T15:31:36.923469Z"
        },
        "id": "1VKKB4y8ET8Q"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "loss_naive, grad_naive = binary_cross_entropy(W, X_val, y_val)\n",
        "print ('loss: %f' % (loss_naive, ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IawKSQqf8vSq"
      },
      "source": [
        "You are provided with a gradient test in the next cells:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-10-29T15:32:14.847516Z",
          "start_time": "2022-10-29T15:32:14.835995Z"
        },
        "id": "nms4Rd7O8s2l"
      },
      "outputs": [],
      "source": [
        "def grad_check(f, x, analytic_grad, num_checks=10, h=1e-5):\n",
        "    for i in range(num_checks):\n",
        "        ix = tuple([randrange(m) for m in x.shape])\n",
        "\n",
        "        oldval = x[ix]\n",
        "        x[ix] = oldval + h # increment by h\n",
        "        fxph = f(x) # evaluate f(x + h)\n",
        "        x[ix] = oldval - h # increment by h\n",
        "        fxmh = f(x) # evaluate f(x - h)\n",
        "        x[ix] = oldval # reset\n",
        "\n",
        "        grad_numerical = (fxph - fxmh) / (2 * h)\n",
        "        grad_analytic = analytic_grad[ix]\n",
        "        rel_error = abs(grad_numerical - grad_analytic) / (abs(grad_numerical) + abs(grad_analytic))\n",
        "        print ('numerical: %f analytic: %f, relative error: %e' % (grad_numerical, grad_analytic, rel_error))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-10-29T15:32:16.311794Z",
          "start_time": "2022-10-29T15:32:16.096941Z"
        },
        "id": "i0OWqVCzET8R"
      },
      "outputs": [],
      "source": [
        "loss, grad = binary_cross_entropy(W, X_val, y_val)\n",
        "f = lambda w: binary_cross_entropy(w, X_val, y_val)[0]\n",
        "grad_numerical = grad_check(f, W, grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-qL6Ti_ET8R"
      },
      "source": [
        "If implemented correctly, the training procedure you already implemented should work."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-10-29T15:32:33.879334Z",
          "start_time": "2022-10-29T15:32:29.754892Z"
        },
        "id": "RlW4q5xjET8R"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "logistic = LogisticRegression(X_train, y_train)\n",
        "loss_history = logistic.train(X_train, y_train, \n",
        "                         learning_rate=1e-7,\n",
        "                         num_iters=1500,\n",
        "                         verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-10-29T15:32:34.608148Z",
          "start_time": "2022-10-29T15:32:34.441343Z"
        },
        "id": "v_6Pf6lYET8R",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "plt.plot(loss_history)\n",
        "plt.xlabel('Iteration number')\n",
        "plt.ylabel('Loss value')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-10-29T15:32:35.262213Z",
          "start_time": "2022-10-29T15:32:35.201690Z"
        },
        "id": "4S2RkwT3ET8R"
      },
      "outputs": [],
      "source": [
        "print(\"Training accuracy: \", logistic.calc_accuracy(X_train, y_train))\n",
        "print(\"Testing accuracy: \", logistic.calc_accuracy(X_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-1hg3RWET8R"
      },
      "source": [
        "## Hyperparameter optimization\n",
        "\n",
        "Your model should have improved from 50% accuracy to ~75% accuracy in a matter of seconds. Now, use the validation set to tune hyperparameters by training different models (using the training dataset) and evaluating the performance using the validation dataset. Save the results in a dictionary mapping tuples of the form `(learning_rate, batch_size)` to tuples of the form `(training_accuracy, validation_accuracy)`. Finally, you should evaluate the best model on the testing dataset. \n",
        "\n",
        "Use a small value for the number of iterations as you develop your code. Once you are confident that everything works, run it again for more iterations. **(5 points)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-10-29T15:34:05.819661Z",
          "start_time": "2022-10-29T15:33:33.339414Z"
        },
        "id": "sxXvTbhJET8R"
      },
      "outputs": [],
      "source": [
        "# You are encouraged to experiment with additional values\n",
        "learning_rates = [1e-7, 5e-6]\n",
        "batch_sizes = [1, 100, 200, 500]\n",
        "\n",
        "results = {}\n",
        "best_val = -1   # The highest validation accuracy that we have seen so far.\n",
        "best_logistic = None # The LogisticRegression object that achieved the highest validation rate.\n",
        "\n",
        "################################################################################\n",
        "#                            START OF YOUR CODE                                #\n",
        "################################################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "################################################################################\n",
        "#                              END OF YOUR CODE                                #\n",
        "################################################################################\n",
        "    \n",
        "# Print out results.\n",
        "for lr, batch_size in sorted(results):\n",
        "    train_accuracy, val_accuracy = results[(lr, batch_size)]\n",
        "    print ('lr %e batch_size %e train accuracy: %f val accuracy: %f' % (\n",
        "                lr, batch_size, train_accuracy, val_accuracy))\n",
        "    \n",
        "print ('best validation accuracy achieved during cross-validation: %f' % best_val)\n",
        "\n",
        "test_accuracy = best_logistic.calc_accuracy(X_test, y_test)\n",
        "print ('Binary logistic regression on raw pixels final test set accuracy: %f' % test_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdRdvKwR7MdF"
      },
      "source": [
        "# The End!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}